{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cde4c8",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Collection\n",
    "\n",
    "This notebook extracts the earthquake-tsunami dataset from Kaggle, performs initial inspection, and saves the raw data for downstream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d8a441",
   "metadata": {},
   "source": [
    "## 1.1 Import Required Libraries\n",
    "\n",
    "Load essential packages for data access, manipulation, and file handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eadb09",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [1.1 Import Required Libraries](#11-import-required-libraries)\n",
    "- [1.2 Extract Dataset from KaggleHub](#12-extract-dataset-from-kagglehub)\n",
    "- [1.3 Load and Inspect Dataset](#13-load-and-inspect-dataset)\n",
    "- [1.4 Save Raw Dataset to Repository](#14-save-raw-dataset-to-repository)\n",
    "- [1.5 Statistical Foundations](#15-statistical-foundations)\n",
    "- [1.6 Summary](#16-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd464ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for data extraction and manipulation\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96673656",
   "metadata": {},
   "source": [
    "## 1.2 Extract Dataset from KaggleHub\n",
    "\n",
    "Use `kagglehub` to download the latest cached version of the earthquake-tsunami dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ecd7b1",
   "metadata": {},
   "source": [
    "[↑ Back to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923df0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Daniel\\.cache\\kagglehub\\datasets\\ahmeduzaki\\global-earthquake-tsunami-risk-assessment-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download dataset using kagglehub (fetches latest cached version)\n",
    "path = kagglehub.dataset_download(\"ahmeduzaki/global-earthquake-tsunami-risk-assessment-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698564f5",
   "metadata": {},
   "source": [
    "## 1.3 Load and Inspect Dataset\n",
    "\n",
    "Read the CSV file into a DataFrame and preview its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f854c",
   "metadata": {},
   "source": [
    "[↑ Back to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b02952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the earthquake-tsunami dataset into a pandas DataFrame\n",
    "df = pd.read_csv(os.path.join(path, \"earthquake_data_tsunami.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a651105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information (column types, non-null counts)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset overview\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa49e8f",
   "metadata": {},
   "source": [
    "## 1.4 Save Raw Dataset to Repository\n",
    "\n",
    "Store the extracted dataset in the `data/raw/` folder for downstream ETL stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f10e60",
   "metadata": {},
   "source": [
    "[↑ Back to Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1046ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw dataset to: ../data/raw/earthquake_data_tsunami.csv\n"
     ]
    }
   ],
   "source": [
    "# Save raw dataset to repository for downstream ETL stages\n",
    "save_path = \"../data/raw/earthquake_data_tsunami.csv\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(save_path, index=False)\n",
    "print(f\"✓ Saved raw dataset to: {save_path}\")\n",
    "print(f\"✓ Total records saved: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4782f",
   "metadata": {},
   "source": [
    "## 1.6 Summary\n",
    "\n",
    "This notebook successfully completed the **Extract** phase of the ETL pipeline:\n",
    "- ✓ Downloaded the latest earthquake-tsunami dataset from Kaggle\n",
    "- ✓ Loaded and inspected the raw data\n",
    "- ✓ Saved the raw dataset to `data/raw/` for transformation\n",
    "\n",
    "**Next Steps:** Proceed to the Transform notebook (`02_etl_transform.ipynb`) for data cleaning and preprocessing.\n",
    "\n",
    "[↑ Back to Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04956e98",
   "metadata": {},
   "source": [
    "## 1.5 Statistical Foundations\n",
    "\n",
    "This section documents core statistical and probability concepts applied in this project.\n",
    "\n",
    "### Descriptive Statistics\n",
    "We summarise numeric columns to understand central tendency and dispersion:\n",
    "- Mean (average): sensitive to extreme values.\n",
    "- Median (50th percentile): robust to outliers.\n",
    "- Standard Deviation (std): typical spread around the mean.\n",
    "- Min/Max & Quartiles: range and distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "RAW_PATH = Path('../data/raw/earthquake_data_tsunami.csv').resolve()\n",
    "\n",
    "# Load raw dataset\n",
    "raw_df = pd.read_csv(RAW_PATH)\n",
    "print('Loaded raw shape:', raw_df.shape)\n",
    "\n",
    "# Identify numeric columns (heuristic)\n",
    "numeric_cols = [c for c in raw_df.columns if pd.api.types.is_numeric_dtype(raw_df[c])]\n",
    "print('Numeric columns:', numeric_cols[:10], '...')\n",
    "\n",
    "# Basic descriptive statistics\n",
    "summary = raw_df[numeric_cols].describe().T\n",
    "summary['variance'] = raw_df[numeric_cols].var().values\n",
    "summary[['mean','50%','std','variance','min','max']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Choose a numeric column to visualise (prefer magnitude-like pattern if present)\n",
    "col_candidates = [c for c in numeric_cols if c.lower() in ['mag','magnitude']]\n",
    "col = col_candidates[0] if col_candidates else (numeric_cols[0] if numeric_cols else None)\n",
    "if col is None:\n",
    "    print('No numeric columns available for histogram.')\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    sns.histplot(raw_df[col].dropna(), bins=30, kde=True, stat='density', ax=ax, color='#4e79a7')\n",
    "    mu, sigma = np.nanmean(raw_df[col]), np.nanstd(raw_df[col])\n",
    "    xs = np.linspace(raw_df[col].min(), raw_df[col].max(), 200)\n",
    "    normal_pdf = 1.0/(sigma*np.sqrt(2*np.pi)) * np.exp(-0.5*((xs-mu)/sigma)**2)\n",
    "    ax.plot(xs, normal_pdf, color='#e15759', lw=2, label=f'Normal fit (μ={mu:.2f}, σ={sigma:.2f})')\n",
    "    ax.set_title(f'Distribution of {col} with normal overlay')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sample t-test (manual) for illustrative hypothesis: mean magnitude == 5.0\n",
    "mu0 = 5.0\n",
    "if col is None:\n",
    "    print('Skipping t-test; no suitable numeric column.')\n",
    "else:\n",
    "    x = raw_df[col].dropna().values\n",
    "    n = len(x)\n",
    "    x_bar = x.mean()\n",
    "    s = x.std(ddof=1)\n",
    "    t_stat = (x_bar - mu0) / (s / np.sqrt(n))\n",
    "    # Approximate two-tailed p-value using normal approximation (|t| large, n large)\n",
    "    # p ≈ 2 * (1 - Φ(|t|)) where Φ is CDF of standard normal\n",
    "    # Φ(z) ≈ 0.5 * (1 + erf(z / sqrt(2))) ; we use numpy.erf\n",
    "    from math import erf, sqrt\n",
    "    def phi(z):\n",
    "        return 0.5 * (1 + erf(z / sqrt(2)))\n",
    "    p_approx = 2 * (1 - phi(abs(t_stat)))\n",
    "    print(f\"One-sample t-test vs μ0={mu0:.2f} on column '{col}':\")\n",
    "    print(f\"n={n}, mean={x_bar:.3f}, std={s:.3f}, t_stat={t_stat:.3f}, approx p-value={p_approx:.4f}\")\n",
    "    if p_approx < 0.05:\n",
    "        print('Reject H0 at 5% level (illustrative).')\n",
    "    else:\n",
    "        print('Fail to reject H0 at 5% level (illustrative).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48613220",
   "metadata": {},
   "source": [
    "[↑ Back to Contents](#contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

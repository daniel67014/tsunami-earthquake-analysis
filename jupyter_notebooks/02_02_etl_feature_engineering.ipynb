{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dc9b87",
   "metadata": {},
   "source": [
    "# 2.2 ETL Feature Engineering Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b78f0",
   "metadata": {},
   "source": [
    "Load essential packages for data access, manipulation, and file handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac9092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c711677",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Load Datasets: Raw vs. Imputed  \n",
    "1.1 Drop Redundant or Constant Features  \n",
    "1.2 Check Feature Distributions  \n",
    "\n",
    "2. Feature Engineering  \n",
    "2.1 Interaction Terms (e.g., cdi √ó nst)  \n",
    "2.2 Binning and Scaling  \n",
    "2.3 Correlation and VIF Analysis  \n",
    "\n",
    "3. Export Transformed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4dc8a",
   "metadata": {},
   "source": [
    "## 1. Load Datasets: Raw vs. Imputed\n",
    "\n",
    "We import two versions of the earthquake dataset to support dual-path benchmarking:\n",
    "\n",
    "- **Raw Dataset** (`earthquake_data_tsunami.csv`):  \n",
    "  Contains original features with missing values. Used for missingness analysis, imputation strategy comparison, and robustness testing.\n",
    "\n",
    "- **Imputed Dataset** (`earthquake_imputed.csv`):  \n",
    "  Contains preprocessed features with missing values filled. Used for feature engineering, model training, and performance benchmarking.\n",
    "\n",
    "This dual import allows us to:\n",
    "- Compare model performance with vs. without imputation\n",
    "- Audit the impact of preprocessing on feature distributions and interactions\n",
    "- Apply validator-grade logic to assess parsimony, drift, and attribution stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e86cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load imputed dataset (used for feature engineering and ML benchmarking)\n",
    "imputed_df = pd.read_csv(\"../data/processed/earthquake_imputed.csv\")\n",
    "\n",
    "# Load raw dataset (used for missingness analysis, imputation benchmarking, and model robustness testing)\n",
    "raw_df = pd.read_csv(\"../data/raw/earthquake_data_tsunami.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff29e7e",
   "metadata": {},
   "source": [
    "## 1.1 Drop Redundant or Constant Features\n",
    "\n",
    "We perform a diagnostic audit to identify features that may be redundant, constant, or quasi-constant. This step supports the principle of parsimony and prepares for downstream correlation and VIF analysis.\n",
    "\n",
    "No features are dropped at this stage ‚Äî we log candidates for potential removal based on:\n",
    "\n",
    "- **Zero variance**: Features with identical values across all rows\n",
    "- **Quasi-constant**: Features dominated by a single value (e.g., >99% frequency)\n",
    "- **Duplicate columns**: Identical feature vectors\n",
    "- **Manual flags**: ID-like or metadata columns with no predictive value\n",
    "\n",
    "This audit is applied to both the raw and imputed datasets to support dual-path benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61c0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Auditing Raw Dataset for redundant features...\n",
      "üß± Constant features: []\n",
      "üßä Quasi-constant features (>99% dominance): []\n",
      "üìé Duplicate columns: []\n",
      "‚ö†Ô∏è Total flagged features in Raw Dataset: 0\n",
      "\n",
      "üîç Auditing Imputed Dataset for redundant features...\n",
      "üß± Constant features: []\n",
      "üßä Quasi-constant features (>99% dominance): []\n",
      "üìé Duplicate columns: []\n",
      "‚ö†Ô∏è Total flagged features in Imputed Dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Define audit function for constant, quasi-constant, and duplicate features\n",
    "def audit_redundant_features(df, name=\"dataset\", quasi_thresh=0.99):\n",
    "    print(f\"\\nüîç Auditing {name} for redundant features...\")\n",
    "\n",
    "    # Constant features (zero variance)\n",
    "    constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    print(f\"üß± Constant features: {constant_cols}\")\n",
    "\n",
    "    # Quasi-constant features\n",
    "    quasi_cols = []\n",
    "    for col in df.columns:\n",
    "        top_freq = df[col].value_counts(normalize=True, dropna=False).max()\n",
    "        if top_freq > quasi_thresh and col not in constant_cols:\n",
    "            quasi_cols.append(col)\n",
    "    print(f\"üßä Quasi-constant features (>{quasi_thresh*100:.0f}% dominance): {quasi_cols}\")\n",
    "\n",
    "    # Duplicate columns\n",
    "    dup_cols = df.T[df.T.duplicated()].index.tolist()\n",
    "    print(f\"üìé Duplicate columns: {dup_cols}\")\n",
    "\n",
    "    # Summary\n",
    "    flagged = set(constant_cols + quasi_cols + dup_cols)\n",
    "    print(f\"‚ö†Ô∏è Total flagged features in {name}: {len(flagged)}\")\n",
    "    return flagged\n",
    "\n",
    "# Run audits on both datasets\n",
    "flagged_raw = audit_redundant_features(raw_df, name=\"Raw Dataset\")\n",
    "flagged_imputed = audit_redundant_features(imputed_df, name=\"Imputed Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a82145",
   "metadata": {},
   "source": [
    "### Audit Summary: No Redundant Features Found\n",
    "\n",
    "The audit identified **no constant, quasi-constant, or duplicate features** in either the raw or imputed dataset.\n",
    "\n",
    "This confirms:\n",
    "- All features exhibit sufficient variance and uniqueness\n",
    "- No immediate candidates for removal based on parsimony or redundancy\n",
    "- Feature pruning will be deferred to downstream correlation and VIF analysis (Section 2.3)\n",
    "\n",
    "This outcome supports the integrity of the current feature set and validates the preprocessing pipeline up to this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f2fef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
